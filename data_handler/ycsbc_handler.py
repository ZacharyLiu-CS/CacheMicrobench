#!/usr/bin/env python3

# This script is used to extract the targeted information from the outpfile.csv generated by intel-pmwatch
# Here is the docs url of intel-pmwatch https://github.com/intel/intel-pmwatch/blob/master/docs/README

import argparse
import subprocess
from pprint import pprint
import re
import logging
import os
from enum import IntEnum
from numpy import *

workload_list = ["workloada", "workloadb",
                 "workloadc", "workloadd",
                 "workloade", "workloadf"]

db_test_case = ["pmrocksdb", "pmemkv", "fastfair",
                "neopmkv", "utree", "listdb",
                "neopmkv(with pbrb)"]

db_name_list = {"pmrocksdb": "pmrocksdb",
                "pmemkv": "pmemkv",
                "fastfair": "fastfair",
                "neopmkv": "neopmkv",
                "utree": "utree",
                "listdb": "utree",
                "neopmkv(with pbrb)": "neopmkv"
                }

db_path_list = {"pmrocksdb": "other_baselines",
                "pmemkv": "other_baselines",
                "fastfair": "other_baselines",
                "neopmkv": "no_pbrb_v1",
                "utree": "other_baselines",
                "listdb": "other_baselines",
                "neopmkv(with pbrb)": "with_pbrb"
                }
thread_count_list = [1, 8]
# workload_list=(workloadd)
value_size_list = ["8B", "1KB"]
head_info = "Each line represents one type of database, and the column data is listed by: a,b,c,d,e,f"


class OPS_UNIT(IntEnum):
    S = 1
    K = 1000
    M = 1000000


logger = logging.getLogger("ycsbc_log_handler")
debug_level = logging.INFO
logger.setLevel(debug_level)

consoleHandler = logging.StreamHandler()
consoleHandler.setLevel(debug_level)
logger.addHandler(consoleHandler)
formatter = logging.Formatter(
    '%(levelname)s: %(message)s')
consoleHandler.setFormatter(formatter)


def extract_data_from_log(log_dir: str,  db_type: str, thread_count: int, value_size: str, workload: str, ops_unit: OPS_UNIT):
    log_file = "{log_dir}/{db_path}/{db_type}_thread_{thread_count}_value_{value_size}_{workload}.log".format(
        log_dir=log_dir, db_path=db_path_list[db_type], db_type=db_name_list[db_type], thread_count=thread_count, value_size=value_size, workload=workload)
    extract_cmd = "cat {log_file} | grep 'Run Perf' | awk '{{print$6}}'".format(
        log_file=log_file)
    if (workload == "workloade"):
        if (db_type == "fastfair" or db_type == "utree" or db_type == "listdb"):
            return 0
    ops_str = os.popen(extract_cmd).read()
    ops = 0
    if ops_str != '':
        ops = round(float(ops_str)/int(ops_unit), 2)
    if ops_str == '':
        logger.warning(log_file + " Don't have results!")
    return ops


def generate_oneline_of_db(log_dir: str, ops_unit: OPS_UNIT, db: str, thread_count: int, value_size: str):
    db_line = []
    for workload in workload_list:
        db_line.append(extract_data_from_log(
            log_dir, db, thread_count, value_size, workload, ops_unit))
    return db_line
    # logger.info("{}:{}".format(db, db_line))


def generate_alllines(log_dir: str, ops_unit: OPS_UNIT, thread_count: int, value_size: str):
    logger.info("Thread: {}, value size: {}".format(thread_count, value_size))
    all_lines = []
    first_line = ['Type']
    first_line.extend([i[-1:].upper() for i in workload_list])
    all_lines.append(first_line)
    for db in db_test_case:
        tmp_line = [db]
        tmp_line.extend(generate_oneline_of_db(
            log_dir, ops_unit, db, thread_count, value_size))
        all_lines.append(tmp_line)
    return all_lines


def write_to_csv(output_path: str, input_data):
    dataout = column_stack(input_data)
    print(dataout)
    savetxt(output_path, dataout, fmt="%10s")


def parse_option():
    """Parse command line opetion"""
    # arg parser
    parser = argparse.ArgumentParser()
    parser.add_argument('--log_dir', '-l', nargs='?', default="../data_sets/ycsbc-output",
                        help='the directory of log files generated by ycsb-c')

    parser.add_argument('--output_dir', '-o', nargs='?',
                        default="../data_plog/ycsbc-plot", help='the directory of handled data output')
    parser.add_argument('--unit_type', '-u', nargs='?', choices=[
                        1, 1000, 1000000], default=1000, help='the output unit types: [1, K, M]')
    args = parser.parse_args()
    # parse args
    log_dir = args.log_dir
    output_dir = args.output_dir
    unit_type = OPS_UNIT(args.unit_type)
    # return args
    return (log_dir, output_dir, unit_type)


if __name__ == "__main__":
    (log_dir, output_dir, ops_unit) = parse_option()
    # thread 1
    all_line = generate_alllines(
        log_dir=log_dir, ops_unit=ops_unit, thread_count=1, value_size="8B")
    write_to_csv("../data_plot/thread_1_valuesize_8B.dat", all_line)

    all_line = generate_alllines(
        log_dir=log_dir, ops_unit=ops_unit, thread_count=8, value_size="8B")
    write_to_csv("../data_plot/thread_8_valuesize_8B.dat", all_line)

    # thread 8
    all_line = generate_alllines(
        log_dir=log_dir, ops_unit=ops_unit, thread_count=1, value_size="1KB")
    write_to_csv("../data_plot/thread_1_valuesize_1KB.dat", all_line)

    all_line = generate_alllines(
        log_dir=log_dir, ops_unit=ops_unit, thread_count=8, value_size="1KB")
    write_to_csv("../data_plot/thread_8_valuesize_1KB.dat", all_line)
